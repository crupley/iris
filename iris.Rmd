---
title: "iris"
author: "crupley"
date: "Wednesday, February 04, 2015"
output: html_document
---

# Introduction

http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data

# Data Exploration

```{r setup, echo=FALSE}
dat <- read.csv("./data/iris.data", header = FALSE)
names(dat) <- c("sepal.length",
                "sepal.width",
                "petal.length",
                "petal.width",
                "class")

library(RColorBrewer)
palette(c(brewer.pal(3, "Set2"), "#E41A1C", "#4DAF4A"))
```

First, let's take a look at the structure of the data. The set contains 150 observations of measurements on 4 different aspects of iris flowers and they are classified as one of 3 different species of iris. There are 50 observations of each species. An example observation of each species from the dataset is shown below.

```{r example, echo=FALSE}
print(dat[c(1, 51, 101),])
```

In order to visualize the data, we can plot the four variables against each other pairwise in a XXX plot.

```{r, initial plot, echo=FALSE}
plot(dat[,-5], col = dat$class, pch = 16, cex = 0.5)
```

```{r, echo=FALSE}
plot(1,1,xaxt = "n",yaxt="n",xlab="",ylab="",type="n")
legend("bottom", levels(dat$class), pch = 16, col = palette()[1:3],
       horiz = TRUE)
```

Looking at the data this way can help show us why this dataset has been has been of interest for clustering data. In each of the plots above, the species of iris group together nicely in their measurements with the possible exception of the sepal length / width of the XXX and XXX species. For simplicity from here forward, I will only look at one of the above plots, petal length vs petal width, though I will continue to use all 4 variables for the analysis.


```{r general functions, echo=FALSE}
source("iris-functions.R")
```

```{r petal plot, echo=FALSE}
plot.iris(dat)
```

# Clustering

```{r 3 plot, fig.height=3}
# initialize parameters
k <- 3  # number of clusters
m <- dim(dat)[1]  # number of examples

par(mfrow = c(1,3))
for(i in 1:3){
    # initialize centroids to k random data points from set
    set.seed(18*i)
    centroid <- dat[sample(1:m, k),-5]
    
    # assign each datapoint to the nearest centroid
    clust <- cluster(dat, centroid)
    
    # plot results
    plot.iris(dat, centroid, clust)
    
    }
par(mfrow = c(1,1))

```

```{r moving centroids, echo=FALSE}

# create a plot with trails from the centroids as they move
set.seed(18)
centroid <- dat[sample(1:m, k),-5]

centroids <- cbind(centroid, number = 1:3)
for(i in 1:10) {
    clust <- cluster(dat, centroid)
    centroid <- update.centroid(dat, clust)
    centroids <- rbind(centroids, cbind(centroid, number = 1:3))
}

plot.iris(dat, centroid, clust)
for(i in 1:3){
    with(centroids[centroids$number == i,],
         lines(petal.length, petal.width,
               col = i, lwd = 2, type = "b", pch = 4))
    }
```

```{r tf plot, echo=FALSE}
dclass <- dat$class
levels(dclass) <- c(1,2,3)

levels(clust) <- class.accuracy(dat$class, clust)[2:4]

tf <- dclass == clust

plot(dat$petal.length, dat$petal.width, col = 5-tf, pch = 16,
     xlab = "Petal Length", ylab = "Petal Width")
legend("bottomright", c("Correctly classified", "Incorrectly classified"),
       pch = 16, col = palette()[5:4])
paste0("Classification accuracy = ",
       round(100*class.accuracy(dat$class, clust)[1],1),"%")
```



```{r accuracy, echo=FALSE}
set.seed(18)
centroid <- dat[sample(1:m, k),-5]
clust <- cluster(dat, centroid)

acc <- c(1, class.accuracy(dat$class, clust)[1])
for(i in 2:10){
    centroid <- update.centroid(dat, clust)
    clust <- cluster(dat, centroid)
    acc <- rbind(acc, c(i, class.accuracy(dat$class, clust)[1]))
}

plot(acc, type = "b", pch = 16, col = 1,
     xlab="Number of loop iterations", ylab="Classification accuracy")
```

```{r calc iterations, echo=FALSE, eval=FALSE}
# placeholder for loop to calculate average # of loops to converge
# was evaluated once and results stored
store <- c(0,0)
for(i in 1:1000){
    print(i)
    centroid <- dat[sample(1:m, k),-5]
    clust <- cluster(dat, centroid)
    acc <- 0
    for(j in 1:10){
        
        acc <- c(acc,class.accuracy(dat$class, clust)[1])
#         print(acc[j+1]); print(j)
        if(abs(acc[j+1]-acc[j]) < 1/200) break
        centroid <- update.centroid(dat, clust)
        clust <- cluster(dat, centroid)
        }
    store <- rbind(store, c(j, acc[j+1]))
    }
store <- store[2:dim(store)[1],]
store <- as.data.frame(store)
names(store) <- c("iterations", "accuracy")
save(store, file = "niter.RData")

```

```{r disp iterations, echo=FALSE}
load("niter.RData")

opar <- par(no.readonly = TRUE)
par(mfrow = c(1,2))

boxplot(store[,1], col = 2, ylab = "Number of Iterations for convergence",
         main = "Histogram of number of iterations required for convergence
         for 1000 randomly initialized sets of centroids")
hist(store[,1], xlab = "Number of Iterations for convergence", col = 2,
     main = "Histogram of number of iterations required for convergence
     for 1000 randomly initialized sets of centroids")
par(opar)

hist(store[,2]*150, 150, xlab = "Number of correctly classified observations
     (out of 150 in sample)", col = 3,
     main = "Histogram of number of correctly classified observations 
     for 1000 randomly initialized sets of centroids")
```

```{r iterations summary}
summary(store)
```


# Appendix: code for functions used



```{r child="iris-functions.R"}
```

```{r echo=FALSE, results='asis'}
# knit the first three lines of first.Rmd
library(knitr)
cat(knit_child(text = readLines('iris-functions.R')[1:3]), sep = '\n')
```
